<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>The Personal Blog of Jirka Maršík</title>
        <link>http://jirka.marsik.me/</link>
        <description><![CDATA[Blogging about research in formal semantics.]]></description>
        <atom:link href="http://jirka.marsik.me//rss.xml" rel="self"
                   type="application/rss+xml" />
        <lastBuildDate>Wed, 26 Feb 2014 00:00:00 UT</lastBuildDate>
        <item>
    <title>My PhD Topic in 3 Minutes</title>
    <link>http://jirka.marsik.me//./2014/02/26/my-phd-topic-in-3-minutes/index.html</link>
    <description><![CDATA[<div class="info">
    Posted on February 26, 2014
    
</div>

<p>During a training seminar on teaching methods, I had the opportunity to work on, prepare and then give a short three-minute presentation of my PhD research that is relatable and engaging enough for a broad audience (essentially the same format as the <a href="http://www.univ-lorraine.fr/180secondes">Ma thèse en 180 secondes</a> competition).</p>
<p>Here is a subtitled recording of my last rendition of my presentation. The credit for the idea of decomposing the link between WORLD and LANGUAGE into WORLD – MODEL – LOGIC – LANGUAGE goes to the great Patrick Blackburn.</p>
<iframe width="640" height="480" src="//www.youtube-nocookie.com/embed/Sn2ixzzwi0I?rel=0" frameborder="0" allowfullscreen></iframe>



<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: THIS CODE IS ONLY AN EXAMPLE * * */
    var disqus_shortname = 'jirkamarsik; // Required - Replace example with your forum shortname
    var disqus_identifier = 'identifier';
    var disqus_title = 'My PhD Topic in 3 Minutes';
    var disqus_url = './2014/02/26/my-phd-topic-in-3-minutes/index.html';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">blog comments powered by <span class="logo-disqus">Disqus</span></a>

]]></description>
    <pubDate>Wed, 26 Feb 2014 00:00:00 UT</pubDate>
    <guid>http://jirka.marsik.me//./2014/02/26/my-phd-topic-in-3-minutes/index.html</guid>
</item>
<item>
    <title>The Working Hipster's Clojure</title>
    <link>http://jirka.marsik.me//./2012/12/19/the-working-hipster-s-clojure/index.html</link>
    <description><![CDATA[<div class="info">
    Posted on December 19, 2012
    
</div>

<p>Lately I have been working on a project for my <em>Applications of NLP</em> class here at Université de Lorraine. The project consists of extracting a Lexicalized <a href="http://en.wikipedia.org/wiki/Tree_adjoining_grammar">Tree Adjoining Grammar</a> from a small corpus of sentences and their semantic representations and then using that grammar to generate new sentences (or hopefully at least some of the original sentences!) from new semantic representations.</p>
<p>However, in order to extract a plausible Tree Adjoining Grammar from a corpus, the trees have to have certain characteristics typical of the ones produced by TAGs (the most important one being that arguments and modifiers are never siblings). Sadly, the parser we used to process our corpora (Stanford Parser) does not yield such pretty trees and we were thus forced to postprocess them. Xia and Palmer propose an algorithm for normalizing parse trees for TAG extraction (<em>From Treebanks to Tree Adjoining Grammars</em>). Instead of implementing their approach, we chose to write our own algorithm, since we had trouble seeing how theirs offers a systematic account for some of the phrase trees in our data and since our fragment of English was quite limited and so were the transformations we needed to perform.</p>
<p>We weren’t sure how the algorithm would work exactly, but we managed to go from nothing to a working solution in the space of one afternoon. Our process was to start with a naive program, run it on the data, query the results to find potential problems, introduce constraints in the algorithm to fix the problems we found and repeat until all problems were resolved. After that, I extended the algorithm to one more phenomenon which we have discovered and tried to make it fit for public display by replacing inside jokes with comments.</p>
<p>I wrote the program in Clojure and since I wanted to post something about how I use the language, I decided to go with this. It’s not a pretty program by far, but throwaway code is code too, so here goes. You can see the final “program” with all of the debugging scaffolding, development code and some documentation thrown in <a href="https://gist.github.com/4323107">here</a>. The program isn’t designed to be compiled and run, but it is rather a collection of definitions useful for building a program that solves my problem. Also, it is dependent on having access to the data I run it on, so you might have a hard time running it on your machines.</p>
<p>I will discuss some of its parts now but instead of focusing on the algorithm itself, I will spend my time talking about some of Clojure’s features and libraries I ended up using (for dubious benefit, I admit) which are quite rare in mainstream languages. Namely, it will be logic programming, metaprogramming facilities and dynamic scoping.</p>
<h2 id="logic-programming">Logic Programming</h2>
<p>At first I was very optimistic about finding a solution to the problem. I imagined formulating the algorithm as a sequence of tree-rewriting rules of a form like this:</p>
<pre><code>(VP
  X
  Y
  (PP Z W))
-&gt;
(VP
  (VP
    X
    Y)
  (PP Z W))</code></pre>
<p>The rad thing about rules like this is that they are dead simple to implement using unification:</p>
<script src="https://gist.github.com/4323409.js"></script>

<p>What is nice is that I get to use unification for checking the input template, capturing values from the input tree and also filling them in to the output template. Sadly, such simple rules have shown to be not flexible enough for our problem. Nevertheless, I still stuck with the idea of using logic programming, my courage bolstered by having just made it through <a href="http://mitpress.mit.edu/books/reasoned-schemer">The Reasoned Schemer</a> without any serious head injury. I feel that it turned out to be a good choice, even though I hit some rough spots with respect to termination.</p>
<script src="https://gist.github.com/4323541.js"></script>

<p>Here is a useful predicate I have defined along the way. It works just like Prolog’s <code>append/3</code>, but it is variadic, meaning it can take an arbitrary number of arguments. This has proven to be very useful in destructuring lists (see <code>fix-flat-npo</code> for <a href="https://gist.github.com/4323107#file-preprocess-clj-L120">example use</a>) given that the alternative (in both Prolog and “vanilla” core.logic) is to introduce lots of new variables. A gotcha I encountered when writing this predicate is that if you write it with the recursive call at the tail and then invoke it with a fresh first argument, the solver will find all the correct solutions, but when asked for more, it will never terminate (Fun, right? OK, by now you might’ve guessed I’m into this whole logic programming thing only because functional programming was getting way too comfortable and mainstream).</p>
<p>One more example.</p>
<script src="https://gist.github.com/4323622.js"></script>

<p>The thing I like about the predicate above is the <code>conde</code> statement in the middle. <code>conde</code> is the core.logic way of writing down disjunctions of goals, but it also has a lot to do with <a href="http://en.wikipedia.org/wiki/Cond"><code>cond</code></a> and branching. The interesting thing about this goal is that it handles two cases which look quite different but share a lot of the same logic. In a functional way, I would have to do some branching to handle the shape of the input and extract the necessary values from it (lines 12 and 14). Then I would have to do some general computation with those values (all the lines outside of the <code>conde</code>) and then branch again, constructing the correct output for both of the input cases (lines 13 and 15). With logic programming, I get the freedom of rearranging my statements without much repercussions (see last example :-)). This allows me to take all my branch dependent code (the input destructuring and output construction) and put it in one place, evading the needlessly complex double bifurcation one would most likely end up with in a functional program (another way to evade the double branching in a functional program would be to abstract the common computations into functions, but given the number of values that need to be computed and their simplicity, this approach would have been quite cumbersome).</p>
<h2 id="metaprogramming-facilities">Metaprogramming Facilities</h2>
<p>In the last example, you might have noticed the strange <code>^::rule</code> incantation up in the function definition. What’s up with that? Well, in my algorithm, there are several ways you could take a phrase tree and do some normalization on it. I define these rules as binary relations between the original tree and the normalized tree. I then want to traverse the input tree bottom-up and try to normalize every node using any rule that fits. For that, I will need a relation that is basically a disjunction over all the defined rules in my system.</p>
<p>One way to do this would be to have a place in your program where you manually call all the rules you have defined one by one, but then you have to update this list of calls everytime you rename/add/remove a rule. Another way would be to create a big data structure and store all the rules inside of it as anonymous functions. Depending on your language, this is probably not the most convenient way to write and test functions. In Clojure, you can have the comfort of defining the rules the way you would have naturally and also of not having to maintain an extra list of calls to your rules, thanks to its metaprogramming facilities. But before we go any further, let’s look at the strange series of glyphs that started this whole discussion.</p>
<p>In Clojure data structures (ergo, also in Clojure code), you can annotate collections and symbols with metadata. Metadata are simple Clojure maps and you can associate them to an item such as a symbol by putting <code>^</code> followed by a metadata map and the symbol you want to annotate, as in <code>^{:how-meta? so-meta} hipster-symbol</code>. A lot of the times, you want metadata entries which serve as boolean flags and so there is syntactic sugar for that, allowing you to write <code>^:approved idol</code> instead of <code>^{:approved true} idol</code> for any keyword, such as <code>:approved</code>. Finally, there is also syntactic sugar for a keyword qualified in the current namespace. If the current namespace is <code>music.genres</code>, you can write <code>::post-indie</code> instead of <code>:music.genres/post-indie</code>. So, by writing <code>^::rule fix-adjunctiono</code> in my namespace <code>grook.preprocess</code>, I am basically saying <code>^{:grook.preprocess/rule true} fix-adjunctiono</code>. This way of prefixing symbols with modifiers might be familiar to you from the access modifiers and type tags of the C language family and indeed, Clojure uses this system for the very same purposes too. The great difference between the hard-coded keywords of those languages and the system used in Clojure is that the Clojure one is completely extensible (just like XML, even with the namespaces (<a href="http://en.wikipedia.org/wiki/Homoiconicity">code is data</a>)).</p>
<p>OK, now back to the metaprogramming part. Whenever you evaluate a <code>def</code> form in Clojure, any metadata you put on the symbol that names the new Var is inherited by the defined Var itself. This Var is a <a href="http://en.wikipedia.org/wiki/Reification_(computer_science)">reified</a> entity inside your program, meaning you can query and work with it as you would with any other value in your program. So, for example, we can write a macro that generates that long boring disjunction for us (OK, the generated disjunction in my program is not that long since in my program, I only have two rules, hence the dubious benefit I talked about in the opening).</p>
<script src="https://gist.github.com/4323906.js"></script>

<p>As a side note, if you want to use this in interactive development, you have to be aware that simply removing a function definition and recompiling your program will not remove previously defined Vars from your running process, meaning that rules which you remove from your source file will still apply. This can be easily fixed by clearing any Vars defined as rules from your namespace before evaluating your definitions (see <a href="https://gist.github.com/4323107#file-preprocess-clj-L8">top of the final program</a>).</p>
<h2 id="dynamic-scoping">Dynamic Scoping</h2>
<p>During the development, we wanted an easy way to see what our program was doing. Specifically, we wanted to have access to the constituents it identified as adjuncts in the context of their parent phrases. I could hardly imagine doing this in a purely functional setting, but luckily, I had the freedom to switch my perspective from the functional model of computation, which has helped me write my algorithm, to the imperative model of computation, where I could start thinking about how my program gets executed and insert the necessary trace statements. To aggregate all the interesting values, I used a global variable, more specifically an Atom, one of Clojure’s reference types.</p>
<p>It soon became obvious that inspecting the tree fragments by themselves was not enough and that some link back to the original tree would be necessary. This proved to be another tricky problem to solve by pure functional programming. I have the names of the trees at one place in the program but then I call some function which calls some other function which calls another function which calls yet another function from which I would then like to access the current tree name. Ideally, I would do something like this, but due to the rules of lexical scoping, it would fail.</p>
<script src="https://gist.github.com/4329199.js"></script>

<p>In imperative languages, this would be solved by storing the current tree name in some ad-hoc instance or global variable. However, Clojure offers us a slightly more elegant solution based on an alternative to lexical scoping known as dynamic scoping.</p>
<p>One way to imagine the process of variable reference resolution in lexical scoping is the following: when the program tries to resolve a variable reference, it starts traversing the blocks of code surrounding the variable reference inside-out until it finds one which declares a variable having the same name. This means that variable references can be resolved by just looking at the source code, making the program easy to reason about. Lexical scoping is also one of the key mechanisms in lambda calculus and gives us nice tricks like closures. However, sometimes we might like to use a different resolution strategy.</p>
<p>Contrast the above description of lexical scoping with that of dynamic scoping: when the program tries to resolve a variable reference, it starts traversing a stack of variable bindings top-to-bottom until it finds one which declares a variable having the same name (you can think of it sort of as the call stack, though function calls don’t have to correspond 1:1 to variable bindings). This means that when you access a variable in dynamic scoping, you don’t necessarily know where it was bound to its current value. This kind of stuff is really wild, but it can be quite useful in controlled doses.</p>
<script src="https://gist.github.com/4329413.js"></script>

<p>Here I define the dynamic (note the metadata keyword) Var <code>*current-tree-name*</code>, which can be thought of almost as declaring that the symbol <code>*current-tree-name*</code> will have dynamic scoping in this namespace. The asterisks around the identifier are called earmuffs and are a common convention when naming dynamically scoped Vars. In every iteration of my tree processing loop, I bind the dynamic Var to a new value, which pushes a new value on to the stack of its bindings which is then accessed in <code>record-adjunct</code>.</p>
<p>We could have done the same thing using a global variable, but even in a simple scenario like this, we can see some advantages to using this more high-level construct. The stack of dynamic bindings is thread-local (as is the call stack) meaning that no other thread will be affected by our new binding. Furthermore, once we leave the scope of the binding, our value gets popped from the stack and the original value is automatically restored, which prevents a class of bugs if we would start manipulating the variable in nested contexts. In summary, the value of <code>*current-tree-name*</code> after running our computation stays the same as it was before and no one else can see a difference in its value while it is running. Since I am using an Atom for the global mutable list of performed adjunctions, I could run this code on several threads at once without any possibility of race conditions and without having to change the code to make it somehow explicitly thread-safe.</p>
<p>Now, there is one more feature of dynamic Vars which I will reveal to you and which will let us solve our problem in a cleaner manner by getting rid of the global variable <code>adjunctions</code>. When a thread-local value has been bound to a dynamic Var, it is possible to use the <code>set!</code> special form to modify its value. This might seem like a cop out given Clojure’s emphasis on immutability (it certainly seems more like a part of its Lisp heritage than its lambda calculus heritage), but it opens new doors for us. More specifically, it enables a kind of bidirectional communication between functions running on the call stack.</p>
<script src="https://gist.github.com/4336670.js"></script>

<p>This program does have some nice properties. We no longer have to worry about the state of a global variable (which becomes a tangible concern even in programs as simple as this if what you want is to keep running new and new code live from the REPL). <code>fix-tree</code> now has a nice interface. It performs the tree normalization and can also report on the adjunctions it performed. Its docstring might say something like this:</p>
<pre><code>If you bind a value to *fix-tree-adjunctions*, a record of every
adjunction performed by fix-tree will be conjed onto it. If
you also bind a value to *fix-tree-current-name*, it will be
incorporated into the generated records.</code></pre>
<p>Now I can use <code>fix-tree</code> on arbitrary trees without it mutilating a global variable that holds the results of my experiments. Parfait!</p>
<p>Well, not parfait yet. We can still make this simpler and therefore prettier. Our habit of passing down the current tree name in a dynamic Var came from the times when we used to store all of the traces in a big global variable. Now it kind of sticks out and doesn’t really make sense. After all, all we really want to do is to just pass a trace of the performed adjunctions back to the top-level caller.</p>
<script src="https://gist.github.com/4336851.js"></script>

<p>Now we have a function that doesn’t muck up any shared state and whose results are determined solely by its inputs. At the same time though, it can easily yield a detailed log of its activity if the caller opts in to it by binding a value to which the log should be appended before calling the function proper. We managed that just by hooking up our code with calls to a very brief logging function instead of having to translate our code into some monad (if your code already was in a monad and all you had to do was to apply a monad transformer, then more power to you :-)).</p>
<p>OK, that’s all I’ve got today. You can find the full program at <a href="https://gist.github.com/4323107">https://gist.github.com/4323107</a>.</p>
<h2 id="further-reading">Further Reading</h2>
<ul>
<li><a href="http://www.clojurebook.com/">Clojure Programming</a>: The most exhaustive book on Clojure I’ve read so far. Beatiful examples, practical under-the-hood details.</li>
<li><a href="http://joyofclojure.com/">The Joy of Clojure</a>: The book that got me into Clojure, soon about to get the second edition treatment.</li>
<li><a href="http://mitpress.mit.edu/books/reasoned-schemer">The Reasoned Schemer</a>: A lovely little book on building a tiny logic programming system in Scheme and implementing in it a relational version of the basics of Lisp and binary arithmetics. Serves as a very good introduction to the miniKanren system which is behind Clojure’s logic programming library, <a href="https://github.com/clojure/core.logic">core.logic</a>.</li>
</ul>

<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: THIS CODE IS ONLY AN EXAMPLE * * */
    var disqus_shortname = 'jirkamarsik; // Required - Replace example with your forum shortname
    var disqus_identifier = 'identifier';
    var disqus_title = 'The Working Hipster's Clojure';
    var disqus_url = './2012/12/19/the-working-hipster-s-clojure/index.html';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">blog comments powered by <span class="logo-disqus">Disqus</span></a>

]]></description>
    <pubDate>Wed, 19 Dec 2012 00:00:00 UT</pubDate>
    <guid>http://jirka.marsik.me//./2012/12/19/the-working-hipster-s-clojure/index.html</guid>
</item>
<item>
    <title>Type Raising and the Cooperative Construction of Meaning</title>
    <link>http://jirka.marsik.me//./2012/12/11/type-raising-and-the-cooperative-construction-of-meaning/index.html</link>
    <description><![CDATA[<div class="info">
    Posted on December 11, 2012
    
</div>

<p>One of the goals of computational semantics is to find an algorithmic way of building representations of natural language utterances that would enable us to perform sound inference on them. Since first-order predicate logic is a well-studied and expressive formal language, we will consider the mapping of natural language utterances onto first-order predicate logic formulas.</p>
<p>In order to build these formulas, we will rely on Frege’s <a href="http://en.wikipedia.org/wiki/Principle_of_compositionality">principle of compositionality</a>. We will assume that our input, the syntactic representation of the utterance, will be in the form of a binary constituency tree. We associate a lambda term to every word of the sentence (a leaf of the constituency tree) using a lexicon. The semantics of an inner node are then obtained by function application, with one of the children acting as functor and the other as argument. We will remark which subconstituent is to be the functor and which the argument, even though this can be easily inferred from the types of the lambda terms. The semantic representation of the sentence is defined as the lambda term associated with the root node.</p>
<h2 id="simple-things-first">Simple Things First</h2>
<p>Let us consider the simplest possible cases first. We will look at proper nouns and intransitive verbs.</p>
<ul>
<li>“Vincent” ↦ <span style="color:blue"><strong>vincent</strong></span> : t</li>
<li>“growls” ↦ <span style="color:red">λx. <strong>growl(</strong>x<strong>)</strong></span> : t → f</li>
</ul>
<p>In the above lexicon, I use the following typographic conventions: “double quotes” for natural language expressions, <strong>bold</strong> for the first-order predicate logic formulas (or their parts) and the unadorned typeface for the lambda calculus specific matter. Later on, I will use expressions in double quotes to refer not only to the expressions themselves, but also to their semantic representations, either computed through composition or obtained from the lexicon. Note that the bold parentheses do not correspond to lambda calculus function applications, they are part of the produced FOPL formulas. I will always mark lambda applications explicitly with the @ symbol. I have also taken the liberty of assigning types to the lambda terms. The two atomic types that come naturally in the domain of FOPL formulas are terms (t) and formulas (f).</p>
<p>The lexicon I show here is very straightforward. We map the proper noun “Vincent” onto a constant and the intransitive verb “growls” onto a lambda abstraction which builds an atomic formula by filling the gap in the unary relation with the supplied argument (we will call functions that take terms and produce formulas predicates).</p>
<p>Let us take the simple sentence “Vincent growls” and its syntactic structure.</p>
<pre><code>(S (NP &quot;Vincent&quot;)
   (VP &quot;growls&quot;))</code></pre>
<p>We obtain the semantic representation of the sentence by applying the “growls” lambda term to the “Vincent” one and β-reducing, <span style="color:red"><strong>growl(<span style="color:blue">vincent</span>)</strong></span> : f.</p>
<p>Let us now consider transitive verbs, as in the sentence “Vincent likes Mia”, with the following lexicon additions</p>
<ul>
<li>“Mia” ↦ <span style="color:green"><strong>mia</strong></span> : t</li>
<li>“likes” ↦ <span style="color:red">λx. λy. <strong>like(</strong>y<strong>,</strong> x<strong>)</strong></span> : t → t → f</li>
</ul>
<p>and syntactic structure.</p>
<pre><code>(S (NP &quot;Vincent&quot;)
   (VP (VT &quot;likes&quot;)
       (NP &quot;Mia&quot;)))</code></pre>
<p>Unsurprisingly, “Mia” maps to a constant and “likes” maps to a binary predicate (a function that builds a formula using two terms as arguments). In the sentence above, “likes” applies to “Mia” first, yielding <span style="color:red">λy. <strong>like(</strong>y<strong>, <span style="color:green">mia</span>)</strong></span> : t → f after β-reduction. Applying this lambda term to that of “Vincent” yields the final representation, <span style="color:red"><strong>like(<span style="color:blue">vincent</span>, <span style="color:green">mia</span>)</strong></span> : f.</p>
<p>So far so good. Some things to note about our system so far. First off, syntactic categories always correspond to a single semantic type (see table below). Second, the <a href="http://en.wikipedia.org/wiki/Head_(linguistics)">head constituent</a> ends up always being the functor during composition.</p>
<ul>
<li>S = f</li>
<li>NP = t</li>
<li>VP = NP → S</li>
<li>VT = NP → VP</li>
</ul>
<h2 id="quantifiable-complications">Quantifiable Complications</h2>
<p>We will now generalize our notion of a noun phrase and admit phrases with determiners such as “a” and “every”. The sentence we would like to be able to analyze is “Every boxer growls”. The desired semantic representation would look something like this <strong><span style="color:green">∀x. (<span style="color:blue">boxer(<span style="color:green">x</span>)</span> → <span style="color:red">growl(<span style="color:green">x</span>)</span>)</span></strong> : f. It is obvious that the relations <strong><span style="color:blue">boxer</span></strong> and <strong><span style="color:red">growl</span></strong> correspond to the words “boxer” and “growls”. What is the semantics of “every” then? Well, as the coloring hints, it is the quantifier, the implication and the variables. However, our old way of composing VPs with NPs will not work here, as one does not simply put a quantifier in the argument position of a relation.</p>
<p>Here are the new elements of our lexicon.</p>
<ul>
<li>“boxer” ↦ <span style="color:blue">λx. <strong>boxer(</strong>x<strong>)</strong></span> : t → f</li>
<li>“every” ↦ <span style="color:green">λP. λQ. <strong>∀x. (</strong>(P@<strong>x</strong>) <strong>→</strong> (Q@<strong>x</strong>)<strong>)</strong></span> : (t → f) → (t → f) → f</li>
</ul>
<p>We represent common nouns as unary predicates (sets of entities are after all a natural denotation for common nouns). As for the quantifier, we map it to a function that builds a universal quantification (see <a href="http://en.wikipedia.org/wiki/Generalized_quantifier">generalized quantifiers</a>). The first argument is the restricting predicate which defines the range of the variable, the second argument is the predicate which is to be true in the domain of quantification.</p>
<p>Calculemus! <a href="http://en.wikipedia.org/wiki/Gottfried_Wilhelm_Leibniz#Symbolic_thought">*</a></p>
<pre><code>(S (NP (Det &quot;Every&quot;)
       (N &quot;boxer&quot;))
   (VP &quot;growls&quot;))</code></pre>
<p>“Every” applies to “boxer” yielding <span style="color:green">λQ. <strong>∀x. (</strong><span style="color:blue"><strong>boxer(<span style="color:green">x</span>)</strong></span> <strong>→</strong> (Q@<strong>x</strong>)<strong>)</strong></span> : (t → f) → f. The resulting NP then applies to “growls” to produce <strong><span style="color:green">∀x. (<span style="color:blue">boxer(<span style="color:green">x</span>)</span> → <span style="color:red">growl(<span style="color:green">x</span>)</span>)</span></strong> : f. Hurray!</p>
<p>OK, what we have here is not as great as it looks. Contrary to the previous situations, it is now the subject NP which acts as the functor, not the VP. However, in our new theory, this is true only for the case when the NP turns out to be a quantified NP. In the case of a proper noun, we still make the VP the functor. This slightly annoying discrepancy has more disturbing implications. The semantic type of one NP (“every boxer” : (t → f) → f)) can be different from that of another NP (“Vincent” : t). This loss of consistency between syntactic and semantic types would make writing the rest of our lexicon difficult as we would always have to consider both cases whenever dealing with NPs.</p>
<p>Luckily, we can solve this problem by raising the types of the proper nouns from t to (t → f) → f. Whenever we have lambda terms F : α → β and G : α, we can construct G’ = λP. P@G : (α → β) → β, which satisfies that F@G = G’<span class="citation">@F</span> (proof via simple β-reduction). What it means is that we can take a functor F and an argument G, do some trivial change to G, and then use the new G as a functor with F as the argument while still getting the same result as before. If we apply this technique to our proper nouns, we get the following updated lexicon entries.</p>
<ul>
<li>“Vincent” ↦ <span style="color:blue">λP. P@<strong>vincent</strong></span> : (t → f) → f</li>
<li>“Mia” ↦ <span style="color:green">λP. P@<strong>mia</strong></span> : (t → f) → f</li>
</ul>
<p>An intuitive way to look at the current NP might be as a thing that ranges over some entities and when given a predicate, produces a formula that is true when the entities being ranged over satisfy the predicate. Therefore, in the above implementations of “Vincent”, the way to produce a formula which is true whenever <span style="color:blue"><strong>vincent</strong></span> satisfies a predicate is to simply use the constant <span style="color:blue"><strong>vincent</strong></span> as the term for the predicate being built.</p>
<h2 id="stuff-gets-difficult">Stuff Gets Difficult</h2>
<p>Let’s try and do a recap of the types we have so far.</p>
<ul>
<li>S = f</li>
<li>N = t → f</li>
<li>VP = t → f</li>
<li>NP = VP → f</li>
<li>VT = ???</li>
</ul>
<p>Oops, we seem to have a problem. By changing what it means to be an NP we have broken our transitive verbs which relied on NPs being terms.</p>
<p>If we were to consider a sentence where the transitive verb has a proper noun as the object (e.g. “Every boxer likes Mia”), things would still work out (we can type our proper nouns as (t → (t → f)) → (t → f)). However, when we try to work with a quantified object, things break.</p>
<p>Let’s use the sentence “Every boxer likes a woman” as an example.</p>
<pre><code>(S (NP (Det &quot;Every&quot;)
       (N &quot;boxer&quot;))
   (VP (TV &quot;likes&quot;)
       (NP (Det &quot;a&quot;)
           (N &quot;woman&quot;))))
    </code></pre>
<ul>
<li>“woman” ↦ <span style="color:teal">λx. <strong>woman(</strong>x<strong>)</strong></span> : t → f</li>
<li>“a” ↦ <span style="color:orange">λP. λQ. <strong>∃y. (</strong>(P@<strong>y</strong>) <strong>∧</strong> (Q@<strong>y</strong>)<strong>)</strong></span> : (t → f) → (t → f) → f</li>
</ul>
<p>The semantics of “woman” are straightforward. As for “a”, we construct another quantifier, this time using an existential quantification. We can construct the meaning of “a woman” as <span style="color:orange">λQ. <strong>∃y. (</strong><span style="color:teal"><strong>woman(<span style="color:orange">y</span>)</strong></span> <strong>∧</strong> (Q@<strong>y</strong>)<strong>)</strong></span> : (t → f) → f. Now, if we try to apply this lambda term to the one of “likes” or vice versa, we get nonsense. In the first case, we end up with a lambda abstraction inside of our FOPL formula, and in the second case, we place (an abstraction of) a quantification as an argument of a relation. Neither of these can ever resolve to a proper formula.</p>
<p>How do we fix this? Well, let us imagine how we would like the meaning of “likes a woman” to look like.</p>
<ul>
<li><span style="color:red">λy. <span style="color:orange"><strong>∃y. (</strong><span style="color:teal"><strong>woman(<span style="color:orange">y</span>)</strong></span> <strong>∧</strong> <span style="color:red"><strong>like(</strong>y<strong>, <span style="color:orange">y</span>)</strong></span><strong>)</strong></span></span> : t → f</li>
</ul>
<p>The above is one reasonable answer (Beware of the two ys! The <span style="color:orange"><strong>orange bold one</strong></span> belongs to the constructed FOPL formula and was contributed by “a”. The <span style="color:red">red plain one</span> is part of the lambda calculus and should belong to “like”.). These semantics for “likes a woman” look good. If we apply them to a term, they yield a formula stating that the entity named by the term does indeed “like a woman”, i.e. what we have is a working predicate.</p>
<p>Now that we know what we want, it is not so difficult to work out the correct version of the “likes” semantics on paper.</p>
<ul>
<li>“likes” ↦ <span style="color:red">λO. λy. O@(λx. <strong>like(</strong>y<strong>,</strong> x<strong>)</strong>)</span> : ((t → f) → f) → t → f</li>
</ul>
<p>OK, that looks good. Let’s check if the whole thing works.</p>
<ul>
<li>“Every boxer” ↦ <span style="color:green">λQ. <strong>∀x. (</strong><span style="color:blue"><strong>boxer(<span style="color:green">x</span>)</strong></span> <strong>→</strong> (Q@<strong>x</strong>)<strong>)</strong></span> : (t → f) → f</li>
<li>“likes a woman” ↦ <span style="color:red">λy. <span style="color:orange"><strong>∃y. (</strong><span style="color:teal"><strong>woman(<span style="color:orange">y</span>)</strong></span> <strong>∧</strong> <span style="color:red"><strong>like(</strong>y<strong>, <span style="color:orange">y</span>)</strong></span><strong>)</strong></span></span> : t → f</li>
<li>“Every boxer”@“likes a woman” ↦ <span style="color:green"><strong>∀x. (<span style="color:blue">boxer(<span style="color:green">x</span>)</span> → <span style="color:red"><span style="color:orange">∃y. (<span style="color:teal">woman(<span style="color:orange">y</span>)</span> ∧ <span style="color:red">like(<span style="color:green">x</span>, <span style="color:orange">y</span>)</span>)</span></span>)</strong></span> : f</li>
</ul>
<p>Yay!</p>
<p>We have a consistent theory of the syntax-semantics interface now, here is our syntax-to-semantics type mapping.</p>
<ul>
<li>S = f</li>
<li>N = t → f</li>
<li>VP = t → f</li>
<li>NP = VP → f</li>
<li>VT = NP → VP</li>
</ul>
<p>The transitive verb is still expressed as a function taking an NP and producing a VP, which seems somewhat linguistically plausible.</p>
<h2 id="the-part-where-i-get-confused-too">The Part Where I Get Confused Too</h2>
<p>OK, so what did I do to my lexicon entry for “likes” that it magically started doing exactly what I needed instead of breaking horribly?</p>
<p>If we look at the type signature of our new “likes” representation and compare it with the old one, we can see that we have swapped out the old type of NP (t) with its new raised type ((t → f) → f). In the body of the abstraction, we are calling our argument O and passing it something which looks like our original value. Isn’t this just like type raising? Why did the meaning of our sentence change then?</p>
<p>Well, it’s not really like type raising if you look at it closely.</p>
<p>
<table>
<tr><td>
this is the original:
</td>
    <td>
<span style="color:red">λx. λy. <strong>like(</strong>y<strong>, </strong>x<strong>)</strong></span>
</td>
    <td> 
: t → t → f
</td>
</tr>
<tr><td>
this would be the type raised version:
</td>
    <td>
<span style="color:red">λO. O@(λx. λy. <strong>like(</strong>y<strong>, </strong>x<strong>)</strong>)</span>
</td>
    <td> 
: ((t → t → f) → (t → f)) → (t → f)
</td>
</tr>
<tr><td>
this is what we used:
</td>
    <td>
<span style="color:red">λO. λy. O@(λx. <strong>like(</strong>y<strong>, </strong>x<strong>)</strong>)</span>
</td>
    <td> 
: ((t → f) → f) → t → f
</td>
</tr>    
</table>
</p>

<p>Now we can see that the position of λy is what is different in the two versions. So what does our version actually do? It’s not that hard to decipher. After a transitive verb is applied to an object, it is supposed to return a predicate, that is a function which takes a term and returns a formula stating something about the term.</p>
<p>Some examples of predicates we have seen:</p>
<ul>
<li><span style="color:red">λx. <strong>growl(</strong>x<strong>)</strong></span> : t → f</li>
<li><span style="color:red">λy. <strong>like(</strong>y<strong>, <span style="color:green">mia</span>)</strong></span> : t → f</li>
<li><span style="color:red">λy. <span style="color:orange"><strong>∃y. (</strong><span style="color:teal"><strong>woman(<span style="color:orange">y</span>)</strong></span> <strong>∧</strong> <span style="color:red"><strong>like(</strong>y<strong>, <span style="color:orange">y</span>)</strong></span><strong>)</strong></span></span> : t → f</li>
</ul>
<p>Let’s dissect our “likes” term then.</p>
<ul>
<li><span style="color:red">λO. λy. O@(λx. <strong>like(</strong>y<strong>, </strong>x<strong>)</strong>)</span> : ((t → f) → f) → t → f</li>
</ul>
<p>First off, we accept the object as an argument and bind its name to the variable <span style="color:red">O</span>. Then we define the predicate we will return. The name of the predicate’s argument will be <span style="color:red">y</span>, this is the variable that the subject will be replacing with its term. Finally, how does the body of the predicate look like? To find out, we ask the object to build it, capturing any term it wants to use to represent itself in the relation using the variable <span style="color:red">x</span>. When <span style="color:red">O</span> is the meaning of “Mia”, the object sends <span style="color:green"><strong>mia</strong></span> as the term to represent it and returns the relation. When <span style="color:red">O</span> is a quantified phrase like “a woman”, it sends the logical variable <span style="color:orange"><strong>y</strong></span> as its representative <span style="color:red">x</span> and it wraps the resulting relation in a quantifier providing a range for <span style="color:orange"><strong>y</strong></span>.</p>
<h3 id="wat">Wat?</h3>
<p>OK, so things got pretty crazy pretty fast. How come we ended up having to repeatedly juggle higher-order functions and raise types? We can find an answer to this by inspecting the complexity (intertwinedness) of the expressions we set out to generate.</p>
<p>To produce <span style="color:red"><strong>growl(<span style="color:blue">vincent</span>)</strong></span> : f, we just had to apply the second-order function “growls” to the first-order constant “Vincent”. This was a fairly trivial thing as the representation of “Vincent” is just plugged into a hole in the represenation of “growls”. Similarly goes for plugging “Vincent” and “Mia” into the binary <span style="color:red"><strong>like</strong></span> relation.</p>
<p>Things get more interesting when we try to generate something like <strong><span style="color:green">∀x. (<span style="color:blue">boxer(<span style="color:green">x</span>)</span> → <span style="color:red">growl(<span style="color:green">x</span>)</span>)</span></strong> : f. When we want to combine “every” and “boxer”, we ask “every” to start. “Every” lays down the quantifiers and asks “boxer” to build the restricting formula, which in turn asks “every” for the term to be filled in the predicate (the variable <span style="color:green"><strong>x</strong></span>). In order to handle this case, we raised the NPs over VPs and ended up with third-order items for the NPs in the lexicon.</p>
<p>Finally, to get quantified noun phrases working as objects of transitive verbs, we had to fix our definition of “likes”, which we raised over the NPs and which made “likes” a fourth-order item in the lexicon. Let’s take a look at applying “likes” to an object such as “a woman”, <span style="color:red">λy. <span style="color:orange"><strong>∃y. (</strong><span style="color:teal"><strong>woman(<span style="color:orange">y</span>)</strong></span> <strong>∧</strong> <span style="color:red"><strong>like(</strong>y<strong>, <span style="color:orange">y</span>)</strong></span><strong>)</strong></span></span> : t → f. Here we ask “likes” to build a term, which begins by putting down the lambda abstraction (<span style="color:red">λy.</span>), then passing control to the object, in this case “a woman”. The object lays down any necessary quantifiers (<strong><span style="color:orange">∃y. <span style="color:teal">woman(<span style="color:orange">y</span>)</span> ∧</span></strong>) and asks “likes” to build the consequent of the quantifier. “Likes” will build the formula (<span style="color:red"><strong>like(</strong>y, …<em>)</em></span>), but not without taking a term to place in the relation from the object (<span style="color:orange"><strong>y</strong></span>). In other words, the function assigned by the lexicon to “likes” is a fourth-order function, which calls the third-order function corresponding to “a woman”, which calls the second-order lambda function defined within “likes” which uses the first-order argument x supplied to it by the object “a woman”.</p>
<p>This kind of there-and-back-again exchange of control is enforced by the interleaved syntactic structure of the FOPL formulas we want to produce (the colors corresponding to the individual lexical items do not form sovereign subtrees) and the principle of compositionality. Each of the two constituents to be combined contains only the information strictly contained in its subtree. To combine these two pieces of information into an elaborate structure, the two lambda terms have to cooperate with each other, exchanging continuation-like lambda functions.</p>
<h2 id="one-last-jump">One Last Jump</h2>
<p>OK, we have gotten our heads around the recursions in the compositions and our pyramid of types, but there is one more thing I’d like to do.</p>
<p>If we look at our theory, there are still some nasty spots left. First off, the type of NP is VP → S, i.e. NPs consume VPs to produce sentences. This sounds quite unintuitive from a linguistic standpoint. We would much rather have a VP which satisfies its valency by consuming an NP as its subject and forms a sentence. Also, there is something off about how the lexical entry for “likes” treats its object fundamentally differently from its subject. Finally, the notion of who gets to be the functor seems kind of arbitrary. Within VPs, it is the verb itself, the head of the phrase. Within NPs, it is the determiner (which is also the head of its phrase, if you <a href="http://www.youtube.com/watch?v=3Dw5opEYprQ">subscribe to the theory of generative grammar</a>). However, within the top-level sentence, it is not VP, the head of the sentence, which is the functor, but the subject NP.</p>
<p>All of these problems are linked and we can solve them in one stroke using a technique we have used several times before. Can you guess which? Yep, it’s type raising. Here are the new lexical entries for our verbs.</p>
<ul>
<li>“growls” ↦ <span style="color:red">λS. S@(λx. <strong>growl(</strong>x<strong>)</strong>)</span> : ((t → f) → f) → f</li>
<li>“likes” ↦ <span style="color:red">λO. λS. S@(λy. O@(λx. <strong>like(</strong>y<strong>,</strong> x<strong>)</strong>))</span> : ((t → f) → f) → ((t → f) → f) → f</li>
</ul>
<p>In the case of the intransitive verb, we simply type raise the VP using the general type raising rule we have used on proper nouns before. For “likes”, it is a little bit more complicated. “Likes” is a transitive verb. That is, it is a function from an NP to a VP. So first, we have to strip away the outer lambda abstraction to get the VP body contained inside and then we mechanically apply the type raising rule on that VP term, changing <span style="color:red">λy. …</span> into <span style="color:red">λS. S@(λy. …)</span>.</p>
<p>Let’s have look at our new type mapping.</p>
<ul>
<li>S = f</li>
<li>N = t → f</li>
<li>NP = (t → f) → f</li>
<li>VP = NP → S</li>
<li>VT = NP → VP</li>
</ul>
<p>Much better! The VP type now actually makes sense linguistically, the functors line up with head constituents, the arguments of transitive verbs are now handled uniformly and we have even made the transitive and intransitive verbs look similar to each other.</p>
<h3 id="minor-caveats">Minor Caveats</h3>
<p>If you try to switch the order of <span style="color:red">S@(λy.</span> and <span style="color:red">O@(λx.</span> in the lexical entry for “likes” (or any other transitive verb for that matter), you will generate a different representation for sentences which have quantifiers in both the subject and object. For example:</p>
<ul>
<li><span style="color:green"><strong>∀x. (<span style="color:blue">boxer(<span style="color:green">x</span>)</span> → <span style="color:red"><span style="color:orange">∃y. (<span style="color:teal">woman(<span style="color:orange">y</span>)</span> ∧ <span style="color:red">like(<span style="color:green">x</span>, <span style="color:orange">y</span>)</span>)</span></span>)</strong></span> : f<br /></li>
<li><span style="color:orange"><strong>∃y. (<span style="color:teal">woman(<span style="color:orange">y</span>)</span> ∧ <span style="color:red"><span style="color:green">∀x. (<span style="color:blue">boxer(<span style="color:green">x</span>)</span> → <span style="color:red">like(<span style="color:green">x</span>, <span style="color:orange">y</span>)</span>)</span></span>)</strong></span> : f</li>
</ul>
<p>These different formulas actually correspond to two different readings of the original sentence “Every boxer likes a woman” (scope ambiguity). The top interpretation is the original one and corresponds to the reading “For every boxer there is a specific woman that he likes”, while the bottom interpretation corresponds to the reading “There is a (one) woman and every boxer likes her”.</p>
<p>This ambiguity can be represented in our theory (maybe somewhat crudely) by providing two different lexical entries for transitive verbs, one for each permissible order of argument qualifiers.</p>
<p>Finally, we should also do something about the FOPL variables we generate. Hard-coding <span style="color:green">x</span> into the lexical entry for “every” and <span style="color:orange">y</span> into the one for “a” doesn’t seem like a good solution, if only because it precludes us from using the same quantifier in the arguments of one verb, e.g. as in “Every boxer likes every woman”.</p>
<h2 id="further-reading">Further Reading</h2>
<ol style="list-style-type: decimal">
<li>Montague, Richard: English as a Formal Language</li>
<li>Montague, Richard: The Proper Treatment of Quantification in Ordinary English</li>
<li>Blackburn, Patrick and Bos, Johan: Representation and Inference in Natural Language</li>
</ol>
<p>I haven’t read any of these, which might explain my state of mild confusion, though I plan to read all of them soon. 1. and 2. are foundational papers in computational semantics which I expect will cover most of what I showed here. 3. is one of the few textbooks in the field.</p>

<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: THIS CODE IS ONLY AN EXAMPLE * * */
    var disqus_shortname = 'jirkamarsik; // Required - Replace example with your forum shortname
    var disqus_identifier = 'identifier';
    var disqus_title = 'Type Raising and the Cooperative Construction of Meaning';
    var disqus_url = './2012/12/11/type-raising-and-the-cooperative-construction-of-meaning/index.html';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">blog comments powered by <span class="logo-disqus">Disqus</span></a>

]]></description>
    <pubDate>Tue, 11 Dec 2012 00:00:00 UT</pubDate>
    <guid>http://jirka.marsik.me//./2012/12/11/type-raising-and-the-cooperative-construction-of-meaning/index.html</guid>
</item>

    </channel> 
</rss>
